{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11456349-e571-44c6-b96f-44a0bfe481d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T10:11:17.906326Z",
     "start_time": "2024-11-19T10:10:54.657687Z"
    }
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import math\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.utils import Secret\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "from haystack import Pipeline\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from pathlib import Path\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "import configparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53e5ed3d-afc6-4cb4-a0f5-02611d369ee8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T15:27:08.208977Z",
     "start_time": "2024-11-05T15:26:38.964002Z"
    }
   },
   "outputs": [],
   "source": [
    "# Función para manejar PDFs localmente\n",
    "def process_local_pdfs(pdf_folder_path=\"recipe_files\", model=\"modelo\"):\n",
    "    \n",
    "    ## Procesa archivos PDF locales en un Document Store.\n",
    "    document_store = InMemoryDocumentStore()\n",
    "    file_type_router = FileTypeRouter(mime_types=[\"application/pdf\"])\n",
    "    pdf_converter = PyPDFToDocument()\n",
    "\n",
    "    ## direcciones de correo\n",
    "    ## borrar referencias\n",
    "    combined_pattern = r\"https?://(?:www\\.)?[^\\s/$.?#].[^\\s]*|\"  # URLs\n",
    "    combined_pattern += r\"\\d{1,4}-\\d{1,4},\\s\\d{4}\\.|\"             # Fechas con guiones\n",
    "    combined_pattern += r\"(?:[A-Z]\\.\\s?)+[A-Z][a-z]+(?:,\\s|$)|\"   # Iniciales seguidas de apellido\n",
    "    combined_pattern += r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}|\"\n",
    "    combined_pattern += r\"(?s)^.*?(Introducti?on|NTRODUCTI?ON)|\"\n",
    "    combined_pattern += r\"(?<=\\bREFERENCES\\b)\\s*[\\s\\S]*$\"\n",
    " \n",
    "    document_cleaner = DocumentCleaner(\n",
    "    remove_repeated_substrings= True,\n",
    "    remove_empty_lines=True,\n",
    "    ascii_only = True,\n",
    "    remove_regex=combined_pattern\n",
    "    )\n",
    "\n",
    "    document_splitter = DocumentSplitter(split_by=\"sentence\", split_length=7, split_overlap=3, split_threshold = 3 )\n",
    "    embedder = SentenceTransformersDocumentEmbedder(model=\"all-mpnet-base-v2\")\n",
    "    document_writer = DocumentWriter(document_store)\n",
    "\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.add_component(instance=file_type_router, name=\"file_type_router\")\n",
    "    pipeline.add_component(instance=pdf_converter, name=\"pdf_converter\")\n",
    "    pipeline.add_component(instance=document_cleaner, name=\"document_cleaner\")\n",
    "    pipeline.add_component(instance=document_splitter, name=\"document_splitter\")\n",
    "    pipeline.add_component(instance=embedder, name=\"document_embeder\")    \n",
    "    pipeline.add_component(instance=document_writer, name=\"document_writer\")\n",
    "\n",
    "    \n",
    "    pipeline.connect(\"file_type_router.application/pdf\", \"pdf_converter.sources\")\n",
    "    pipeline.connect(\"pdf_converter\",  \"document_cleaner\")\n",
    "    pipeline.connect(\"document_cleaner\", \"document_splitter\")\n",
    "    pipeline.connect(\"document_splitter\", \"document_embeder\")\n",
    "    pipeline.connect(\"document_embeder\", \"document_writer\")\n",
    "    \n",
    "    \n",
    "\n",
    "    pdf_files = list(Path(pdf_folder_path).glob(\"**/*.pdf\"))\n",
    "\n",
    "    pipeline.run({\"file_type_router\": {\"sources\": pdf_files}})\n",
    "\n",
    "    return document_store\n",
    "\n",
    "# Procesar PDFs locales y crear embeddings\n",
    "document_store = process_local_pdfs(\"pdfs/\",\"hol\")\n",
    "all_documents = document_store.filter_documents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbe9ae07-034c-4e98-b35b-0769411e66b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T15:27:08.225654Z",
     "start_time": "2024-11-05T15:27:08.210141Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0215f4c-3ce6-4cf7-8772-0f74b893d2f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T15:27:08.229386Z",
     "start_time": "2024-11-05T15:27:08.226603Z"
    }
   },
   "outputs": [],
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7457a8b9-b750-4b05-b458-d7dd59754567",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T15:27:08.243413Z",
     "start_time": "2024-11-05T15:27:08.231205Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extraer información de cada `Document` en `all_docs`\n",
    "data = []\n",
    "for doc in all_documents:\n",
    "    data.append({\n",
    "        \"id\": doc.id,\n",
    "        \"content\": doc.content,\n",
    "        \"vector\": doc.embedding,\n",
    "        \"file_path\": doc.meta.get(\"file_path\"),\n",
    "        \"source_id\": doc.meta.get(\"source_id\"),\n",
    "        \"page_number\": doc.meta.get(\"page_number\"),\n",
    "        \"split_id\": doc.meta.get(\"split_id\"),\n",
    "        \"split_idx_start\": doc.meta.get(\"split_idx_start\")\n",
    "    })\n",
    "    \n",
    "\n",
    "# Convertir a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Guardar en un archivo CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13979cef-69ec-4494-a6ab-fc51669bbbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['file_path'] = df['file_path'].apply(lambda x: x[5:-4])  # Eliminar las primeras 5 letras y las últimas 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0526e423-22b1-476c-9027-4398815a9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'file_path': 'eid'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "05e4ee94-3d77-4095-a88c-7a5e193c22de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65259480-fe96-4bf4-8b47-803b21b7795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"fragmentos_documentos.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d861faee6c97c56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T14:19:15.973260Z",
     "start_time": "2024-11-05T14:19:15.933259Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fragmentos_documentos.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "898314161607962a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T14:19:18.222657Z",
     "start_time": "2024-11-05T14:19:18.215446Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34be8521d89ffc6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T15:27:08.730297Z",
     "start_time": "2024-11-05T15:27:08.244408Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"word_count\"] = df[\"content\"].apply(lambda x: len(str(x).split()))\n",
    "# Genera el histograma\n",
    "plt.hist(df[\"word_count\"], bins=range(1, df[\"word_count\"].max() + 1), edgecolor=\"black\")\n",
    "plt.xlabel(\"Cantidad de palabras\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Histograma de la cantidad de palabras por fila\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419906d4",
   "metadata": {},
   "source": [
    "Mayores a 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6a52079",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMas250 = df[df[\"word_count\"] > 250]\n",
    "dfMas250.to_csv(\"dfMas250.csv\", index=False, encoding=\"utf-8\")\n",
    "len(dfMas250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300a455",
   "metadata": {},
   "source": [
    "Menores a 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65044eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMenos30 = df[df[\"word_count\"] < 40]\n",
    "dfMenos30.to_csv(\"dfMenos30.csv\", index=False, encoding=\"utf-8\")\n",
    "len(dfMenos30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a83eb",
   "metadata": {},
   "source": [
    "Entre 30 y 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aedbe64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEntre30y250 = df[(df[\"word_count\"] >= 30) & (df[\"word_count\"] <= 250)]\n",
    "dfEntre30y250.to_csv(\"dfEntre30y250.csv\", index=False, encoding=\"utf-8\")\n",
    "len(dfEntre30y250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1870b-14a4-43fc-ae65-f5bb6da06342",
   "metadata": {},
   "source": [
    "## Creación de base de datos vectorial en hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b582eb97-059f-4943-b521-a94772f515ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "import os\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"variables.ini\")   \n",
    "\n",
    "# Autentícate con tu token de acceso de Hugging Face\n",
    "HF_Token = config['DEFAULT']['HF_Token']\n",
    "login(HF_Token)\n",
    "\n",
    "\n",
    "# Define el nombre del repositorio\n",
    "HF_Usuario = config['DEFAULT']['HF_Usuario']\n",
    "\n",
    "repo_name = \"fragmentos_documentos_61_all-mpnet-base-v2\"  # Cambia esto por el nombre que deseas para tu dataset\n",
    "repo_id = f\"{HF_Usuario}/{repo_name}\"\n",
    "\n",
    "# Instancia la API de Hugging Face\n",
    "api = HfApi()\n",
    "\n",
    "# Crear el repositorio en Hugging Face Hub\n",
    "api.create_repo(repo_id=repo_id, repo_type=\"dataset\", exist_ok=True)\n",
    "\n",
    "# Sube el archivo CSV al repositorio creado\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"fragmentos_documentos.csv\",\n",
    "    path_in_repo=\"fragmentos_documentos.csv\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dac610-4e6b-4418-8c10-c15e04387520",
   "metadata": {},
   "source": [
    "### MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f56a76fb-c6ce-4e9b-90df-9a2ae110eb1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T10:15:57.073129Z",
     "start_time": "2024-11-19T10:15:57.070870Z"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "document = \"\"\"Stability assessments are\n",
    "usually performed by means of ofﬂine time-domain simula-\n",
    "tions (TDS) [11], following a worst-case approach, perform-\n",
    "ing simulations for only a limited set of relevant operating\n",
    "points (OP) and contingencies. These critical conditions and\n",
    "contingencies are usually selected based on the historical\n",
    "performance of the system and the experience of the operator\n",
    "and planner [12]. For instance, frequency problems are more\n",
    "likely to arise during periods of low net load, where only a\n",
    "limited number of SGs are available to support frequency\n",
    "response. The critical contingency considered for the simu-\n",
    "lations is the sudden outage of the largest online generation\n",
    "unit [13]. In this context, stability assessments of large power\n",
    "systems with thousands of buses and hundreds of genera-\n",
    "tors and contingencies, and for a large number of typically\n",
    "encountered operating conditions are not realistically feasi-\n",
    "ble, especially for real-time, online stability assessments, due\n",
    "to computational limitations [14].\n",
    "Although worst-case scenarios used for assessing system\n",
    "stability are usually well deﬁned, in power systems dom-\n",
    "inated by CGTs, traditional approaches for deﬁning these\n",
    "scenarios may no longer be valid. With high levels of CGTs,\n",
    "power system dynamics change in new ways, thus making the\n",
    "process of deﬁning worst-case scenarios even more challeng-\n",
    "ing. Additionally, the high uncertainty of CGTs may not only\n",
    "result in a shift of the critical operating conditions, but also in\n",
    "an increase of the number of risky conditions in which sys-\n",
    "tem stability may be threaten [15]. Consequently, currently\n",
    "widely accepted criteria for deﬁning critical scenarios for\n",
    "stability assessments may fail to cover all critical operating\n",
    "points and contingencies that might result in power system\n",
    "instabilities [13].\"\"\"\n",
    "\n",
    "document = document.replace(\" ´\", \"\")\n",
    "document = unicodedata.normalize('NFKD', document).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "# Eliminar saltos de línea y caracteres innecesarios\n",
    "document = re.sub(r'\\s+', ' ', document)  # Sustituye múltiples espacios y saltos por uno solo\n",
    "\n",
    "\n",
    "\n",
    "document = re.sub(r'[^A-Za-z0-9\\s.,:;()\\[\\]-]','', document)  # Elimina caracteres especiales\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "You are an expert in knowledge engineering and will give you a {Document},\n",
    "          First, with that information identify the RDF triples (subject, predicate, object) of each relevant sentence.\n",
    "          Second, Normalizes/lemmatizes the text of the elements identified in each triple.\n",
    "          Third, Recognizes the entities on subject and object and provide the category of each one, e.g.subject and object and normalizes text.\n",
    "          Fourth, Only returns the output in csv format. Don't explain the process.\n",
    "          Format:\n",
    "          Subject,predicate,object,category of subject, category of object.\n",
    "          Example:\n",
    "          Tim Berner-Lee,interestIn,Fog Computing,PERSON,TOPIC\n",
    "          Document to be analyze:\\n\n",
    "\"\"\" + document\n",
    "\n",
    "prompt2 = \"\"\"\n",
    "You are an expert in knowledge engineering and will give you a Document to by analyze.\n",
    "          First, from the document's content identifies or recognizes an Named entities only if exist, \n",
    "          such as PERSON, ORGANIZATION, TOPIC/SUBJECT, etc.\n",
    "          Second, For relevant entities identify the underlying RDF triples in which them are implied \n",
    "          (subject, predicate, object).\n",
    "          Third, Normalizes/lemmatizes the text of the elements identified in each triple.\n",
    "          Fourth, Recognizes the entities on subject and object and provide the category of each one, \n",
    "          e.g.subject and object and normalizes text.\n",
    "          fifth, Only returns the output in csv format if not exist any returno NO. Don't explain the process.\n",
    "          Format:\n",
    "          Subject,predicate,object,category of subject, category of object.\n",
    "          Example:\n",
    "          Tim Berner-Lee,interestIn,Fog Computing,PERSON,TOPIC\n",
    "          Document to be analyze:\\n\n",
    "\"\"\" + document\n",
    "\n",
    "prompt3 = \"\"\"\n",
    "        You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph only return that information about IA.\n",
    "        Your task is to identify the entities and relations specified in the user prompt from a given text and produce the output in JSON format.\n",
    "        This output should be a list of JSON objects, with each object containing the following keys:\n",
    "\n",
    "        •⁠  ⁠*\"head\"*: The text of the extracted entity, which must match one of the types specified in the user prompt.\n",
    "        •⁠  ⁠*\"head_type\"*: The type of the extracted head entity, selected from the specified list of types.\n",
    "        •⁠  ⁠*\"relation\"*: The type of relation between the \"head\" and the \"tail,\" chosen from the list of allowed relations.\n",
    "        •⁠  ⁠*\"tail\"*: The text of the entity representing the tail of the relation.\n",
    "        •⁠  ⁠*\"tail_type\"*: The type of the tail entity, also selected from the provided list of types.\n",
    "\n",
    "        Extract as many entities and relationships as possible.\n",
    "\n",
    "        *Entity Consistency*: Ensure consistency in entity representation. If an entity, like \"John Doe,\" appears multiple times in the text under different names or pronouns (e.g., \"Joe,\" \"he\"), use the most complete identifier consistently. \n",
    "\n",
    "        *Important Notes*:\n",
    "        •⁠  ⁠Do not add any extra explanations or text.\n",
    "        •⁠  ⁠If no one relation can be obtained only return NO\n",
    "\n",
    "        allowed_nodes = [\"Person\", \"Organization\", \"Location\", \"Method\", \"ResearchField\",\"Technology\",\"Metric\",\"DataSet\",\"Group\",\"Disease\"]\n",
    "\n",
    "        \n",
    "        examples = [\n",
    "    {\n",
    "        \"head\": \"Adam\",\n",
    "        \"head_type\": \"Person\",\n",
    "        \"relation\": \"WORKS_FOR\",\n",
    "        \"tail\": \"Microsoft\",\n",
    "        \"tail_type\": \"Company\",\n",
    "    },\n",
    "    {\n",
    "        \"head\": \"Adam\",\n",
    "        \"head_type\": \"Person\",\n",
    "        \"relation\": \"HAS_AWARD\",\n",
    "        \"tail\": \"Best Talent\",\n",
    "        \"tail_type\": \"Award\",\n",
    "    }\n",
    "]\n",
    "Document:\n",
    "\"\"\" + document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d650e67",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "01ca8cb5-cb5a-47f2-a9a4-7e49aca484f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T10:20:21.433303Z",
     "start_time": "2024-11-19T10:20:21.412648Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "api_key = \"Ingrese su API key\"##\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 1024,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-pro\",\n",
    "  generation_config=generation_config,\n",
    ")\n",
    "\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"parts\": [\n",
    "          \"\"\"\n",
    "          You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph only return that information about IA.\n",
    "        Your task is to identify the entities and relations specified in the user prompt from a given text and produce the output in JSON format.\n",
    "        This output should be a list of JSON objects, with each object containing the following keys:\n",
    "\n",
    "        •⁠  ⁠*\"head\"*: The text of the extracted entity, which must match one of the types specified in the user prompt.\n",
    "        •⁠  ⁠*\"head_type\"*: The type of the extracted head entity, selected from the specified list of types.\n",
    "        •⁠  ⁠*\"relation\"*: The type of relation between the \"head\" and the \"tail,\" chosen from the list of allowed relations.\n",
    "        •⁠  ⁠*\"tail\"*: The text of the entity representing the tail of the relation.\n",
    "        •⁠  ⁠*\"tail_type\"*: The type of the tail entity, also selected from the provided list of types.\n",
    "\n",
    "        Extract as many entities and relationships as possible.\n",
    "\n",
    "        *Entity Consistency*: Ensure consistency in entity representation. If an entity, like \"John Doe,\" appears multiple times in the text under different names or pronouns (e.g., \"Joe,\" \"he\"), use the most complete identifier consistently. \n",
    "\n",
    "        *Important Notes*:\n",
    "        •⁠  ⁠Do not add any extra explanations or text.\n",
    "        •⁠  ⁠If no one relation can be optained only return NO\n",
    "\n",
    "        allowed_nodes = [\"Person\", \"Organization\", \"Location\", \"Method\", \"ResearchField\",\"Technology\",\"Metric\",\"DataSet\",\"Group\",\"Disease\"]\n",
    "\n",
    "        \n",
    "        examples = [\n",
    "            {\n",
    "                \"head\": \"Adam\",\n",
    "                \"head_type\": \"Person\",\n",
    "                \"relation\": \"WORKS_FOR\",\n",
    "                \"tail\": \"Microsoft\",\n",
    "                \"tail_type\": \"Company\",\n",
    "            },\n",
    "            {\n",
    "                \"head\": \"Adam\",\n",
    "                \"head_type\": \"Person\",\n",
    "                \"relation\": \"HAS_AWARD\",\n",
    "                \"tail\": \"Best Talent\",\n",
    "                \"tail_type\": \"Award\",\n",
    "            }\n",
    "        ]\n",
    "        Document:\n",
    "          \"\"\"\n",
    "      ],\n",
    "    },\n",
    "  ]\n",
    ")\n",
    "\n",
    "response = chat_session.send_message(document)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f5948-aa39-4bbd-941e-ca968234d959",
   "metadata": {},
   "source": [
    "## Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff60bd-2aa2-41ee-a792-e4bb41cdc002",
   "metadata": {},
   "source": [
    "Pruebassss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97dac250-d9c2-4e43-9287-1e8d3328ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "\tbase_url=\"https://api-inference.huggingface.co/v1/\",\n",
    "\tapi_key=\"\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": prompt3\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion1 = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=1024,\n",
    "    temperature= 0.1\n",
    "    \n",
    ")\n",
    "\n",
    "output = completion1.choices[0].message.content  # Extrae el contenido del mensaje\n",
    "jsonString = output.replace(\"\\\\n\", \"\\n\")  # Reemplaza los caracteres \"\\n\" por saltos de línea reales\n",
    "\n",
    "print(jsonString)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38c20c-bad3-4b6e-b699-7cf8b183c542",
   "metadata": {},
   "source": [
    "## Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd32bc53-4e33-41d4-9518-59cf14c7fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "\tbase_url=\"https://api-inference.huggingface.co/v1/\",\n",
    "\tapi_key=\"h\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": prompt3\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion2 = client.chat.completions.create(\n",
    "    model=\"google/gemma-1.1-2b-it\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500\n",
    ")\n",
    "\n",
    "output = completion2.choices[0].message.content  # Extrae el contenido del mensajeformatted_output = output.replace(\"\\\\n\", \"\\n\")  # Reemplaza los caracteres \"\\n\" por saltos de línea reales\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6487f7",
   "metadata": {},
   "source": [
    "openai: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\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa880a0-be09-4a24-9dea-583d5903d87d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
